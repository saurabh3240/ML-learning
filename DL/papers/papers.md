# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

[Jacob Devlin](https://arxiv.org/search/cs?searchtype=author&query=Devlin%2C+J), [Ming-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+M), [Kenton Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K), [Kristina Toutanova](https://arxiv.org/search/cs?searchtype=author&query=Toutanova%2C+K)

learning sources:https://www.youtube.com/watch?v=0EtD5ybnh_s

## WORD EMBEDDINGS

- slide a window across the windows are similar to each other or related in some way
- 1 Bn Words 

### ByTe Pair encoding

- sentence piece paper

### Transformers

Unsupervised language Task

-  predict next word
- predict missing word
- detect sentence and phrase switch



# ATTENTION IS ALL YOU NEED

- CNN for Text 