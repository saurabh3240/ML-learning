{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# N : batch size\n",
    "# D_in : input Dimensions\n",
    "# D_out : output Dimensions\n",
    "# H : Hidden Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (64, 1000)\n",
      "y.shape (64, 10)\n"
     ]
    }
   ],
   "source": [
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "print(\"x.shape\",x.shape)\n",
    "print(\"y.shape\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.shape (1000, 100)\n",
      "w2.shape (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "print(\"w1.shape\", w1.shape)\n",
    "print(\"w2.shape\", w2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.shape (64, 100)\n",
      "h_relu.shape (64, 100)\n",
      "y_pred.shape (64, 10)\n",
      "0 23289381.961685423\n",
      "grad_y_pred.shape (64, 10)\n",
      "grad_w2 (100, 10)\n",
      "grad_h_relu (64, 100)\n",
      "grad_w1 (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(1):\n",
    "    # Forward Pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    print(\"h.shape\", h.shape)\n",
    "    print(\"h_relu.shape\", h_relu.shape)\n",
    "    print(\"y_pred.shape\", y_pred.shape)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # compute and print loss\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backpropogation to compute gradient of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0*(y_pred -y)\n",
    "    print(\"grad_y_pred.shape\", grad_y_pred.shape)\n",
    "\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    print(\"grad_w2\", grad_w2.shape)\n",
    "    \n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    print(\"grad_h_relu\", grad_h_relu.shape)\n",
    "    \n",
    "    grad_h= grad_h_relu.copy()\n",
    "    grad_h[h <0] = 0\n",
    "    \n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    print(\"grad_w1\", grad_w1.shape)\n",
    "    \n",
    "    # update weights \n",
    "    w1 -= learning_rate* grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DEEP]",
   "language": "python",
   "name": "conda-env-DEEP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
