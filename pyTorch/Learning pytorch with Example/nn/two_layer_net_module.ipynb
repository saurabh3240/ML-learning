{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Custom nn Modules\n",
    "--------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation defines the model as a custom Module subclass. Whenever you\n",
    "want a model more complex than a simple sequence of existing Modules you will\n",
    "need to define your model this way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min =0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(size_average = True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9655946493148804\n",
      "1 0.965490996837616\n",
      "2 0.9653887748718262\n",
      "3 0.9652856588363647\n",
      "4 0.9651826024055481\n",
      "5 0.9650802612304688\n",
      "6 0.9649775624275208\n",
      "7 0.9648748636245728\n",
      "8 0.9647719264030457\n",
      "9 0.9646687507629395\n",
      "10 0.9645661115646362\n",
      "11 0.964464545249939\n",
      "12 0.9643615484237671\n",
      "13 0.9642587900161743\n",
      "14 0.9641561508178711\n",
      "15 0.9640539288520813\n",
      "16 0.9639504551887512\n",
      "17 0.9638481140136719\n",
      "18 0.9637454748153687\n",
      "19 0.9636428952217102\n",
      "20 0.963540256023407\n",
      "21 0.9634381532669067\n",
      "22 0.9633353352546692\n",
      "23 0.9632333517074585\n",
      "24 0.9631308317184448\n",
      "25 0.9630283117294312\n",
      "26 0.9629265666007996\n",
      "27 0.9628232717514038\n",
      "28 0.9627203941345215\n",
      "29 0.9626185297966003\n",
      "30 0.9625163078308105\n",
      "31 0.9624133110046387\n",
      "32 0.9623115658760071\n",
      "33 0.9622087478637695\n",
      "34 0.9621056318283081\n",
      "35 0.9620047807693481\n",
      "36 0.9619023203849792\n",
      "37 0.961800754070282\n",
      "38 0.9616984128952026\n",
      "39 0.961596667766571\n",
      "40 0.9614948034286499\n",
      "41 0.9613922834396362\n",
      "42 0.9612910151481628\n",
      "43 0.9611890912055969\n",
      "44 0.9610875844955444\n",
      "45 0.960985541343689\n",
      "46 0.9608837366104126\n",
      "47 0.9607815742492676\n",
      "48 0.9606790542602539\n",
      "49 0.9605774879455566\n",
      "50 0.9604759216308594\n",
      "51 0.9603744745254517\n",
      "52 0.9602726697921753\n",
      "53 0.9601715207099915\n",
      "54 0.9600699543952942\n",
      "55 0.9599677324295044\n",
      "56 0.9598667025566101\n",
      "57 0.9597655534744263\n",
      "58 0.9596635699272156\n",
      "59 0.9595627784729004\n",
      "60 0.9594612121582031\n",
      "61 0.9593595266342163\n",
      "62 0.9592587351799011\n",
      "63 0.9591568112373352\n",
      "64 0.9590563774108887\n",
      "65 0.9589546918869019\n",
      "66 0.9588528871536255\n",
      "67 0.9587523341178894\n",
      "68 0.9586519002914429\n",
      "69 0.9585501551628113\n",
      "70 0.95844966173172\n",
      "71 0.958348274230957\n",
      "72 0.9582473635673523\n",
      "73 0.9581462144851685\n",
      "74 0.9580451846122742\n",
      "75 0.9579439163208008\n",
      "76 0.9578428268432617\n",
      "77 0.9577423334121704\n",
      "78 0.9576422572135925\n",
      "79 0.9575402140617371\n",
      "80 0.9574400186538696\n",
      "81 0.9573381543159485\n",
      "82 0.9572374224662781\n",
      "83 0.9571365118026733\n",
      "84 0.9570358991622925\n",
      "85 0.956934928894043\n",
      "86 0.9568344950675964\n",
      "87 0.9567334055900574\n",
      "88 0.9566327929496765\n",
      "89 0.9565322995185852\n",
      "90 0.9564312696456909\n",
      "91 0.9563308954238892\n",
      "92 0.9562298059463501\n",
      "93 0.9561287760734558\n",
      "94 0.9560292959213257\n",
      "95 0.9559281468391418\n",
      "96 0.9558270573616028\n",
      "97 0.9557272791862488\n",
      "98 0.955626368522644\n",
      "99 0.955525279045105\n",
      "100 0.9554241895675659\n",
      "101 0.9553241729736328\n",
      "102 0.9552232623100281\n",
      "103 0.9551224708557129\n",
      "104 0.9550223350524902\n",
      "105 0.954922080039978\n",
      "106 0.9548214077949524\n",
      "107 0.9547212719917297\n",
      "108 0.9546205401420593\n",
      "109 0.9545207023620605\n",
      "110 0.9544199705123901\n",
      "111 0.9543201327323914\n",
      "112 0.9542193412780762\n",
      "113 0.9541195034980774\n",
      "114 0.9540188908576965\n",
      "115 0.9539192318916321\n",
      "116 0.9538183212280273\n",
      "117 0.9537184834480286\n",
      "118 0.9536179304122925\n",
      "119 0.9535179138183594\n",
      "120 0.9534180760383606\n",
      "121 0.9533178210258484\n",
      "122 0.9532173275947571\n",
      "123 0.9531172513961792\n",
      "124 0.9530169367790222\n",
      "125 0.9529168009757996\n",
      "126 0.9528171420097351\n",
      "127 0.9527174830436707\n",
      "128 0.9526170492172241\n",
      "129 0.952517032623291\n",
      "130 0.9524164199829102\n",
      "131 0.9523165822029114\n",
      "132 0.9522171020507812\n",
      "133 0.9521170854568481\n",
      "134 0.9520174860954285\n",
      "135 0.9519174695014954\n",
      "136 0.9518173336982727\n",
      "137 0.9517182111740112\n",
      "138 0.9516181945800781\n",
      "139 0.9515177011489868\n",
      "140 0.9514178037643433\n",
      "141 0.9513183832168579\n",
      "142 0.9512186050415039\n",
      "143 0.9511189460754395\n",
      "144 0.9510191679000854\n",
      "145 0.9509191513061523\n",
      "146 0.9508205652236938\n",
      "147 0.9507201910018921\n",
      "148 0.9506213068962097\n",
      "149 0.9505208134651184\n",
      "150 0.9504212141036987\n",
      "151 0.9503218531608582\n",
      "152 0.9502221941947937\n",
      "153 0.9501225352287292\n",
      "154 0.9500236511230469\n",
      "155 0.9499241709709167\n",
      "156 0.9498246908187866\n",
      "157 0.9497243165969849\n",
      "158 0.9496256709098816\n",
      "159 0.9495253562927246\n",
      "160 0.9494268298149109\n",
      "161 0.949327290058136\n",
      "162 0.9492284059524536\n",
      "163 0.9491289258003235\n",
      "164 0.9490290880203247\n",
      "165 0.948930561542511\n",
      "166 0.9488312005996704\n",
      "167 0.9487317204475403\n",
      "168 0.9486321210861206\n",
      "169 0.9485330581665039\n",
      "170 0.9484335780143738\n",
      "171 0.9483343958854675\n",
      "172 0.9482353925704956\n",
      "173 0.9481358528137207\n",
      "174 0.9480370283126831\n",
      "175 0.9479371905326843\n",
      "176 0.9478393793106079\n",
      "177 0.9477397203445435\n",
      "178 0.9476401209831238\n",
      "179 0.9475410580635071\n",
      "180 0.9474424123764038\n",
      "181 0.9473429918289185\n",
      "182 0.9472441673278809\n",
      "183 0.9471445083618164\n",
      "184 0.947046160697937\n",
      "185 0.9469476938247681\n",
      "186 0.9468479156494141\n",
      "187 0.9467488527297974\n",
      "188 0.9466499090194702\n",
      "189 0.9465513229370117\n",
      "190 0.9464520215988159\n",
      "191 0.9463523626327515\n",
      "192 0.9462536573410034\n",
      "193 0.9461551904678345\n",
      "194 0.9460565447807312\n",
      "195 0.9459578394889832\n",
      "196 0.9458588361740112\n",
      "197 0.9457603693008423\n",
      "198 0.9456607699394226\n",
      "199 0.9455621838569641\n",
      "200 0.9454632997512817\n",
      "201 0.9453642964363098\n",
      "202 0.9452658891677856\n",
      "203 0.9451676607131958\n",
      "204 0.9450677633285522\n",
      "205 0.9449692964553833\n",
      "206 0.9448705911636353\n",
      "207 0.944771409034729\n",
      "208 0.9446736574172974\n",
      "209 0.9445748329162598\n",
      "210 0.944476306438446\n",
      "211 0.9443771243095398\n",
      "212 0.9442781209945679\n",
      "213 0.944179356098175\n",
      "214 0.9440807104110718\n",
      "215 0.9439816474914551\n",
      "216 0.9438825845718384\n",
      "217 0.9437840580940247\n",
      "218 0.9436857104301453\n",
      "219 0.9435866475105286\n",
      "220 0.9434884786605835\n",
      "221 0.943388819694519\n",
      "222 0.9432905912399292\n",
      "223 0.943191409111023\n",
      "224 0.9430937767028809\n",
      "225 0.9429952502250671\n",
      "226 0.9428966641426086\n",
      "227 0.9427975416183472\n",
      "228 0.9426984786987305\n",
      "229 0.9425995945930481\n",
      "230 0.9425015449523926\n",
      "231 0.9424031972885132\n",
      "232 0.9423046112060547\n",
      "233 0.9422059059143066\n",
      "234 0.9421079754829407\n",
      "235 0.9420096278190613\n",
      "236 0.9419107437133789\n",
      "237 0.9418128132820129\n",
      "238 0.9417142868041992\n",
      "239 0.9416161775588989\n",
      "240 0.9415174722671509\n",
      "241 0.9414187669754028\n",
      "242 0.9413210153579712\n",
      "243 0.9412228465080261\n",
      "244 0.9411241412162781\n",
      "245 0.9410260319709778\n",
      "246 0.9409272074699402\n",
      "247 0.9408301115036011\n",
      "248 0.9407315254211426\n",
      "249 0.9406335949897766\n",
      "250 0.9405360221862793\n",
      "251 0.9404371380805969\n",
      "252 0.940339207649231\n",
      "253 0.9402419328689575\n",
      "254 0.9401429295539856\n",
      "255 0.9400448799133301\n",
      "256 0.939947247505188\n",
      "257 0.9398487210273743\n",
      "258 0.9397504925727844\n",
      "259 0.9396533966064453\n",
      "260 0.9395553469657898\n",
      "261 0.9394572377204895\n",
      "262 0.9393590688705444\n",
      "263 0.9392614364624023\n",
      "264 0.9391635656356812\n",
      "265 0.9390654563903809\n",
      "266 0.9389666318893433\n",
      "267 0.9388688802719116\n",
      "268 0.9387707710266113\n",
      "269 0.9386736154556274\n",
      "270 0.9385760426521301\n",
      "271 0.9384785890579224\n",
      "272 0.9383803606033325\n",
      "273 0.9382826089859009\n",
      "274 0.9381846189498901\n",
      "275 0.9380861520767212\n",
      "276 0.9379894137382507\n",
      "277 0.937891960144043\n",
      "278 0.9377935528755188\n",
      "279 0.9376959800720215\n",
      "280 0.9375981092453003\n",
      "281 0.9375007748603821\n",
      "282 0.9374025464057922\n",
      "283 0.9373046159744263\n",
      "284 0.9372071027755737\n",
      "285 0.937109649181366\n",
      "286 0.9370120763778687\n",
      "287 0.9369145631790161\n",
      "288 0.9368170499801636\n",
      "289 0.9367189407348633\n",
      "290 0.9366216659545898\n",
      "291 0.9365242123603821\n",
      "292 0.9364268183708191\n",
      "293 0.9363292455673218\n",
      "294 0.9362319111824036\n",
      "295 0.9361344575881958\n",
      "296 0.9360364675521851\n",
      "297 0.9359383583068848\n",
      "298 0.9358415603637695\n",
      "299 0.9357441067695618\n",
      "300 0.9356468915939331\n",
      "301 0.9355490803718567\n",
      "302 0.9354516267776489\n",
      "303 0.9353541135787964\n",
      "304 0.9352574348449707\n",
      "305 0.9351599812507629\n",
      "306 0.9350622892379761\n",
      "307 0.9349654316902161\n",
      "308 0.9348679780960083\n",
      "309 0.9347704648971558\n",
      "310 0.9346734285354614\n",
      "311 0.9345759153366089\n",
      "312 0.9344784021377563\n",
      "313 0.934381365776062\n",
      "314 0.9342843890190125\n",
      "315 0.9341877102851868\n",
      "316 0.9340906143188477\n",
      "317 0.9339933395385742\n",
      "318 0.9338962435722351\n",
      "319 0.9337986707687378\n",
      "320 0.9337016940116882\n",
      "321 0.9336045980453491\n",
      "322 0.9335070848464966\n",
      "323 0.9334109425544739\n",
      "324 0.9333135485649109\n",
      "325 0.9332171678543091\n",
      "326 0.93312007188797\n",
      "327 0.933022677898407\n",
      "328 0.9329261779785156\n",
      "329 0.9328289031982422\n",
      "330 0.932732105255127\n",
      "331 0.9326346516609192\n",
      "332 0.9325373768806458\n",
      "333 0.9324411153793335\n",
      "334 0.9323440790176392\n",
      "335 0.9322469830513\n",
      "336 0.9321500658988953\n",
      "337 0.9320533871650696\n",
      "338 0.9319570660591125\n",
      "339 0.9318599700927734\n",
      "340 0.9317631721496582\n",
      "341 0.9316661953926086\n",
      "342 0.9315692782402039\n",
      "343 0.9314725995063782\n",
      "344 0.9313753247261047\n",
      "345 0.9312790036201477\n",
      "346 0.9311816096305847\n",
      "347 0.9310863614082336\n",
      "348 0.9309899210929871\n",
      "349 0.9308934211730957\n",
      "350 0.9307964444160461\n",
      "351 0.9306999444961548\n",
      "352 0.9306046366691589\n",
      "353 0.9305079579353333\n",
      "354 0.9304114580154419\n",
      "355 0.9303141832351685\n",
      "356 0.9302188754081726\n",
      "357 0.9301222562789917\n",
      "358 0.930025577545166\n",
      "359 0.9299303293228149\n",
      "360 0.9298340678215027\n",
      "361 0.9297378659248352\n",
      "362 0.9296417236328125\n",
      "363 0.9295455813407898\n",
      "364 0.9294496774673462\n",
      "365 0.9293531179428101\n",
      "366 0.9292570352554321\n",
      "367 0.9291622042655945\n",
      "368 0.9290651082992554\n",
      "369 0.928970217704773\n",
      "370 0.9288736581802368\n",
      "371 0.928777813911438\n",
      "372 0.9286817312240601\n",
      "373 0.9285857081413269\n",
      "374 0.9284898042678833\n",
      "375 0.9283944368362427\n",
      "376 0.9282976984977722\n",
      "377 0.9282022714614868\n",
      "378 0.928106427192688\n",
      "379 0.9280106425285339\n",
      "380 0.9279154539108276\n",
      "381 0.9278208017349243\n",
      "382 0.9277245402336121\n",
      "383 0.9276293516159058\n",
      "384 0.9275331497192383\n",
      "385 0.9274380803108215\n",
      "386 0.9273417592048645\n",
      "387 0.9272459745407104\n",
      "388 0.9271506071090698\n",
      "389 0.9270556569099426\n",
      "390 0.9269598722457886\n",
      "391 0.926864743232727\n",
      "392 0.9267691373825073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 0.9266740083694458\n",
      "394 0.9265782237052917\n",
      "395 0.9264830350875854\n",
      "396 0.9263880848884583\n",
      "397 0.9262923002243042\n",
      "398 0.9261962175369263\n",
      "399 0.926101803779602\n",
      "400 0.9260061979293823\n",
      "401 0.9259108304977417\n",
      "402 0.9258154630661011\n",
      "403 0.9257208704948425\n",
      "404 0.9256254434585571\n",
      "405 0.9255297780036926\n",
      "406 0.9254347085952759\n",
      "407 0.9253396987915039\n",
      "408 0.9252440333366394\n",
      "409 0.9251489639282227\n",
      "410 0.9250540733337402\n",
      "411 0.9249591827392578\n",
      "412 0.9248638153076172\n",
      "413 0.9247690439224243\n",
      "414 0.9246742129325867\n",
      "415 0.9245792627334595\n",
      "416 0.9244837760925293\n",
      "417 0.9243882298469543\n",
      "418 0.9242933988571167\n",
      "419 0.9241987466812134\n",
      "420 0.9241039156913757\n",
      "421 0.9240094423294067\n",
      "422 0.9239141345024109\n",
      "423 0.9238198399543762\n",
      "424 0.9237247705459595\n",
      "425 0.9236294031143188\n",
      "426 0.9235346913337708\n",
      "427 0.9234403371810913\n",
      "428 0.9233449101448059\n",
      "429 0.9232503771781921\n",
      "430 0.9231551885604858\n",
      "431 0.9230605959892273\n",
      "432 0.9229663014411926\n",
      "433 0.9228718876838684\n",
      "434 0.9227768778800964\n",
      "435 0.9226819276809692\n",
      "436 0.9225873947143555\n",
      "437 0.9224926233291626\n",
      "438 0.9223980903625488\n",
      "439 0.9223029017448425\n",
      "440 0.9222089648246765\n",
      "441 0.9221142530441284\n",
      "442 0.9220202565193176\n",
      "443 0.9219252467155457\n",
      "444 0.9218313097953796\n",
      "445 0.9217362403869629\n",
      "446 0.9216418266296387\n",
      "447 0.9215472936630249\n",
      "448 0.9214528203010559\n",
      "449 0.921358585357666\n",
      "450 0.9212648272514343\n",
      "451 0.9211695790290833\n",
      "452 0.9210753440856934\n",
      "453 0.9209817051887512\n",
      "454 0.9208868741989136\n",
      "455 0.9207925796508789\n",
      "456 0.9206985235214233\n",
      "457 0.9206041097640991\n",
      "458 0.920509934425354\n",
      "459 0.9204154014587402\n",
      "460 0.9203221201896667\n",
      "461 0.9202275276184082\n",
      "462 0.9201338887214661\n",
      "463 0.9200395345687866\n",
      "464 0.9199457168579102\n",
      "465 0.9198516011238098\n",
      "466 0.9197570085525513\n",
      "467 0.9196632504463196\n",
      "468 0.9195691347122192\n",
      "469 0.9194751977920532\n",
      "470 0.9193806648254395\n",
      "471 0.9192863702774048\n",
      "472 0.9191932678222656\n",
      "473 0.9190994501113892\n",
      "474 0.919005274772644\n",
      "475 0.9189110994338989\n",
      "476 0.918817400932312\n",
      "477 0.9187239408493042\n",
      "478 0.9186290502548218\n",
      "479 0.9185358285903931\n",
      "480 0.9184419512748718\n",
      "481 0.9183481335639954\n",
      "482 0.9182543754577637\n",
      "483 0.9181603193283081\n",
      "484 0.918066680431366\n",
      "485 0.9179729223251343\n",
      "486 0.9178789258003235\n",
      "487 0.9177853465080261\n",
      "488 0.9176920652389526\n",
      "489 0.9175980687141418\n",
      "490 0.9175046682357788\n",
      "491 0.9174111485481262\n",
      "492 0.9173175692558289\n",
      "493 0.9172232747077942\n",
      "494 0.9171293377876282\n",
      "495 0.9170357584953308\n",
      "496 0.9169424176216125\n",
      "497 0.9168483018875122\n",
      "498 0.9167550802230835\n",
      "499 0.9166616201400757\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(t, loss.item())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
